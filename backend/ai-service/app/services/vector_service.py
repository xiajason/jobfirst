"""
JobFirst å‘é‡æœåŠ¡
å¤„ç†å‘é‡æ•°æ®åº“æ“ä½œã€ç›¸ä¼¼æ€§æœç´¢ç­‰åŠŸèƒ½
"""

import asyncio
import logging
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime
import numpy as np
from dataclasses import dataclass

import asyncpg
from app.config import config
from app.utils.logger import get_logger
from app.utils.cache import AsyncCache

logger = get_logger(__name__)


@dataclass
class VectorSearchResult:
    """å‘é‡æœç´¢ç»“æœ"""
    content_id: str
    content_type: str
    similarity_score: float
    metadata: Dict[str, Any]
    embedding: Optional[List[float]] = None


class VectorService:
    """å‘é‡æœåŠ¡ä¸»ç±»"""
    
    def __init__(self):
        self.db_pool: Optional[asyncpg.Pool] = None
        self.cache = AsyncCache()
        self.health_status = "initializing"
        self.vector_dimensions = config.vector.VECTOR_DIMENSIONS
        self.similarity_threshold = config.vector.VECTOR_SIMILARITY_THRESHOLD
        
    async def initialize(self):
        """åˆå§‹åŒ–å‘é‡æœåŠ¡"""
        try:
            # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥æ± 
            self.db_pool = await asyncpg.create_pool(
                host=config.database.HOST,
                port=config.database.PORT,
                user=config.database.USER,
                password=config.database.PASSWORD,
                database=config.database.NAME,
                min_size=5,
                max_size=config.database.POOL_SIZE
            )
            
            # éªŒè¯æ•°æ®åº“è¿æ¥
            async with self.db_pool.acquire() as conn:
                await conn.execute("SELECT 1")
            
            # åˆå§‹åŒ–å‘é‡æ‰©å±•
            await self._init_vector_extension()
            
            # åˆ›å»ºå‘é‡è¡¨
            await self._create_vector_tables()
            
            # åˆ›å»ºå‘é‡ç´¢å¼•
            await self._create_vector_indexes()
            
            self.health_status = "healthy"
            logger.info("âœ… å‘é‡æœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.health_status = "error"
            logger.error(f"âŒ å‘é‡æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            if self.db_pool:
                await self.db_pool.close()
            logger.info("âœ… å‘é‡æœåŠ¡èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ å‘é‡æœåŠ¡èµ„æºæ¸…ç†å¤±è´¥: {e}")
    
    def is_healthy(self) -> bool:
        """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
        return self.health_status == "healthy"
    
    async def _init_vector_extension(self):
        """åˆå§‹åŒ–PostgreSQLå‘é‡æ‰©å±•"""
        async with self.db_pool.acquire() as conn:
            # æ£€æŸ¥pgvectoræ‰©å±•æ˜¯å¦å·²å®‰è£…
            result = await conn.fetchval(
                "SELECT COUNT(*) FROM pg_extension WHERE extname = 'vector'"
            )
            
            if result == 0:
                logger.info("ğŸ“¦ å®‰è£…pgvectoræ‰©å±•...")
                await conn.execute("CREATE EXTENSION IF NOT EXISTS vector")
                logger.info("âœ… pgvectoræ‰©å±•å®‰è£…å®Œæˆ")
            else:
                logger.info("âœ… pgvectoræ‰©å±•å·²å­˜åœ¨")
    
    async def _create_vector_tables(self):
        """åˆ›å»ºå‘é‡ç›¸å…³è¡¨"""
        async with self.db_pool.acquire() as conn:
            # åˆ›å»ºå‘é‡åµŒå…¥è¡¨
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS vector_embeddings (
                    id SERIAL PRIMARY KEY,
                    content_id VARCHAR(255) NOT NULL,
                    content_type VARCHAR(50) NOT NULL,
                    embedding vector(%s) NOT NULL,
                    metadata JSONB DEFAULT '{}',
                    model_version VARCHAR(100),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(content_id, content_type)
                )
            """, self.vector_dimensions)
            
            # åˆ›å»ºç®€å†å‘é‡è¡¨
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS resume_vectors (
                    id SERIAL PRIMARY KEY,
                    resume_id VARCHAR(255) UNIQUE NOT NULL,
                    embedding vector(%s) NOT NULL,
                    content_hash VARCHAR(64) NOT NULL,
                    analysis_data JSONB DEFAULT '{}',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """, self.vector_dimensions)
            
            # åˆ›å»ºèŒä½å‘é‡è¡¨
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS job_vectors (
                    id SERIAL PRIMARY KEY,
                    job_id VARCHAR(255) UNIQUE NOT NULL,
                    embedding vector(%s) NOT NULL,
                    content_hash VARCHAR(64) NOT NULL,
                    job_data JSONB DEFAULT '{}',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """, self.vector_dimensions)
            
            # åˆ›å»ºå…¬å¸å‘é‡è¡¨
            await conn.execute("""
                CREATE TABLE IF NOT EXISTS company_vectors (
                    id SERIAL PRIMARY KEY,
                    company_id VARCHAR(255) UNIQUE NOT NULL,
                    embedding vector(%s) NOT NULL,
                    content_hash VARCHAR(64) NOT NULL,
                    company_data JSONB DEFAULT '{}',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """, self.vector_dimensions)
            
            logger.info("âœ… å‘é‡è¡¨åˆ›å»ºå®Œæˆ")
    
    async def _create_vector_indexes(self):
        """åˆ›å»ºå‘é‡ç´¢å¼•"""
        async with self.db_pool.acquire() as conn:
            # ä¸ºç®€å†å‘é‡åˆ›å»ºç´¢å¼•
            await conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_resume_vectors_embedding 
                ON resume_vectors 
                USING ivfflat (embedding vector_cosine_ops)
                WITH (lists = 100)
            """)
            
            # ä¸ºèŒä½å‘é‡åˆ›å»ºç´¢å¼•
            await conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_job_vectors_embedding 
                ON job_vectors 
                USING ivfflat (embedding vector_cosine_ops)
                WITH (lists = 100)
            """)
            
            # ä¸ºé€šç”¨å‘é‡åµŒå…¥åˆ›å»ºç´¢å¼•
            await conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_vector_embeddings_embedding 
                ON vector_embeddings 
                USING ivfflat (embedding vector_cosine_ops)
                WITH (lists = 100)
            """)
            
            logger.info("âœ… å‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆ")
    
    async def store_resume_embedding(
        self,
        resume_id: str,
        embedding: List[float],
        content_hash: str,
        analysis_data: Optional[Dict[str, Any]] = None,
        model_version: str = "text-embedding-ada-002"
    ) -> bool:
        """å­˜å‚¨ç®€å†å‘é‡åµŒå…¥"""
        try:
            async with self.db_pool.acquire() as conn:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = await conn.fetchval(
                    "SELECT id FROM resume_vectors WHERE resume_id = $1",
                    resume_id
                )
                
                if existing:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    await conn.execute("""
                        UPDATE resume_vectors 
                        SET embedding = $1, content_hash = $2, analysis_data = $3, updated_at = CURRENT_TIMESTAMP
                        WHERE resume_id = $4
                    """, embedding, content_hash, analysis_data or {}, resume_id)
                else:
                    # æ’å…¥æ–°è®°å½•
                    await conn.execute("""
                        INSERT INTO resume_vectors (resume_id, embedding, content_hash, analysis_data)
                        VALUES ($1, $2, $3, $4)
                    """, resume_id, embedding, content_hash, analysis_data or {})
                
                # åŒæ—¶å­˜å‚¨åˆ°é€šç”¨å‘é‡è¡¨
                await self._store_generic_embedding(
                    resume_id, "resume", embedding, 
                    {"content_hash": content_hash, "analysis_data": analysis_data},
                    model_version
                )
                
                logger.info(f"âœ… ç®€å†å‘é‡å­˜å‚¨æˆåŠŸ: {resume_id}")
                return True
                
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨ç®€å†å‘é‡å¤±è´¥: {e}")
            return False
    
    async def store_job_embedding(
        self,
        job_id: str,
        embedding: List[float],
        content_hash: str,
        job_data: Optional[Dict[str, Any]] = None,
        model_version: str = "text-embedding-ada-002"
    ) -> bool:
        """å­˜å‚¨èŒä½å‘é‡åµŒå…¥"""
        try:
            async with self.db_pool.acquire() as conn:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                existing = await conn.fetchval(
                    "SELECT id FROM job_vectors WHERE job_id = $1",
                    job_id
                )
                
                if existing:
                    # æ›´æ–°ç°æœ‰è®°å½•
                    await conn.execute("""
                        UPDATE job_vectors 
                        SET embedding = $1, content_hash = $2, job_data = $3, updated_at = CURRENT_TIMESTAMP
                        WHERE job_id = $4
                    """, embedding, content_hash, job_data or {}, job_id)
                else:
                    # æ’å…¥æ–°è®°å½•
                    await conn.execute("""
                        INSERT INTO job_vectors (job_id, embedding, content_hash, job_data)
                        VALUES ($1, $2, $3, $4)
                    """, job_id, embedding, content_hash, job_data or {})
                
                # åŒæ—¶å­˜å‚¨åˆ°é€šç”¨å‘é‡è¡¨
                await self._store_generic_embedding(
                    job_id, "job", embedding, 
                    {"content_hash": content_hash, "job_data": job_data},
                    model_version
                )
                
                logger.info(f"âœ… èŒä½å‘é‡å­˜å‚¨æˆåŠŸ: {job_id}")
                return True
                
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨èŒä½å‘é‡å¤±è´¥: {e}")
            return False
    
    async def _store_generic_embedding(
        self,
        content_id: str,
        content_type: str,
        embedding: List[float],
        metadata: Dict[str, Any],
        model_version: str
    ) -> bool:
        """å­˜å‚¨é€šç”¨å‘é‡åµŒå…¥"""
        try:
            async with self.db_pool.acquire() as conn:
                await conn.execute("""
                    INSERT INTO vector_embeddings (content_id, content_type, embedding, metadata, model_version)
                    VALUES ($1, $2, $3, $4, $5)
                    ON CONFLICT (content_id, content_type) 
                    DO UPDATE SET 
                        embedding = EXCLUDED.embedding,
                        metadata = EXCLUDED.metadata,
                        model_version = EXCLUDED.model_version,
                        updated_at = CURRENT_TIMESTAMP
                """, content_id, content_type, embedding, metadata, model_version)
                
                return True
                
        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨é€šç”¨å‘é‡å¤±è´¥: {e}")
            return False
    
    async def similarity_search(
        self,
        query_embedding: List[float],
        content_type: str,
        limit: int = 10,
        similarity_threshold: Optional[float] = None
    ) -> List[VectorSearchResult]:
        """ç›¸ä¼¼æ€§æœç´¢"""
        try:
            if similarity_threshold is None:
                similarity_threshold = self.similarity_threshold
            
            async with self.db_pool.acquire() as conn:
                # ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦æœç´¢
                query = """
                    SELECT 
                        content_id,
                        content_type,
                        metadata,
                        embedding,
                        1 - (embedding <=> $1) as similarity_score
                    FROM vector_embeddings 
                    WHERE content_type = $2 
                        AND 1 - (embedding <=> $1) >= $3
                    ORDER BY embedding <=> $1
                    LIMIT $4
                """
                
                rows = await conn.fetch(
                    query, 
                    query_embedding, 
                    content_type, 
                    similarity_threshold, 
                    limit
                )
                
                results = []
                for row in rows:
                    result = VectorSearchResult(
                        content_id=row['content_id'],
                        content_type=row['content_type'],
                        similarity_score=float(row['similarity_score']),
                        metadata=row['metadata'],
                        embedding=row['embedding'].tolist() if row['embedding'] else None
                    )
                    results.append(result)
                
                logger.info(f"âœ… ç›¸ä¼¼æ€§æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
                return results
                
        except Exception as e:
            logger.error(f"âŒ ç›¸ä¼¼æ€§æœç´¢å¤±è´¥: {e}")
            return []
    
    async def resume_job_match(
        self,
        resume_id: str,
        limit: int = 10,
        similarity_threshold: Optional[float] = None
    ) -> List[VectorSearchResult]:
        """ç®€å†ä¸èŒä½åŒ¹é…"""
        try:
            # è·å–ç®€å†å‘é‡
            resume_embedding = await self._get_resume_embedding(resume_id)
            if not resume_embedding:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ç®€å†å‘é‡: {resume_id}")
                return []
            
            # æœç´¢ç›¸ä¼¼èŒä½
            results = await self.similarity_search(
                query_embedding=resume_embedding,
                content_type="job",
                limit=limit,
                similarity_threshold=similarity_threshold
            )
            
            logger.info(f"âœ… ç®€å†èŒä½åŒ¹é…å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªåŒ¹é…èŒä½")
            return results
            
        except Exception as e:
            logger.error(f"âŒ ç®€å†èŒä½åŒ¹é…å¤±è´¥: {e}")
            return []
    
    async def semantic_search(
        self,
        query: str,
        content_type: str,
        limit: int = 10,
        similarity_threshold: Optional[float] = None
    ) -> List[VectorSearchResult]:
        """è¯­ä¹‰æœç´¢ï¼ˆéœ€è¦å…ˆè½¬æ¢ä¸ºå‘é‡ï¼‰"""
        try:
            # è¿™é‡Œéœ€è¦è°ƒç”¨AIæœåŠ¡ç”ŸæˆæŸ¥è¯¢å‘é‡
            # æš‚æ—¶è¿”å›ç©ºç»“æœï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦é›†æˆAIæœåŠ¡
            logger.info(f"ğŸ” è¯­ä¹‰æœç´¢: {query} -> {content_type}")
            
            # TODO: é›†æˆAIæœåŠ¡ç”ŸæˆæŸ¥è¯¢å‘é‡
            # query_embedding = await ai_service.generate_embeddings(query)
            # return await self.similarity_search(query_embedding, content_type, limit, similarity_threshold)
            
            return []
            
        except Exception as e:
            logger.error(f"âŒ è¯­ä¹‰æœç´¢å¤±è´¥: {e}")
            return []
    
    async def _get_resume_embedding(self, resume_id: str) -> Optional[List[float]]:
        """è·å–ç®€å†å‘é‡"""
        try:
            async with self.db_pool.acquire() as conn:
                row = await conn.fetchrow(
                    "SELECT embedding FROM resume_vectors WHERE resume_id = $1",
                    resume_id
                )
                
                if row:
                    return row['embedding'].tolist()
                return None
                
        except Exception as e:
            logger.error(f"âŒ è·å–ç®€å†å‘é‡å¤±è´¥: {e}")
            return None
    
    async def get_vector_stats(self) -> Dict[str, Any]:
        """è·å–å‘é‡ç»Ÿè®¡ä¿¡æ¯"""
        try:
            async with self.db_pool.acquire() as conn:
                stats = {}
                
                # ç»Ÿè®¡å„ç±»å‹å‘é‡æ•°é‡
                for content_type in ['resume', 'job', 'company']:
                    count = await conn.fetchval(
                        "SELECT COUNT(*) FROM vector_embeddings WHERE content_type = $1",
                        content_type
                    )
                    stats[f"{content_type}_count"] = count
                
                # ç»Ÿè®¡æ€»å‘é‡æ•°é‡
                total_count = await conn.fetchval(
                    "SELECT COUNT(*) FROM vector_embeddings"
                )
                stats['total_count'] = total_count
                
                # ç»Ÿè®¡å­˜å‚¨å¤§å°ï¼ˆä¼°ç®—ï¼‰
                size_query = """
                    SELECT pg_size_pretty(pg_total_relation_size('vector_embeddings')) as size
                """
                size_row = await conn.fetchrow(size_query)
                stats['storage_size'] = size_row['size'] if size_row else 'Unknown'
                
                # ç»Ÿè®¡æœ€è¿‘æ›´æ–°æ—¶é—´
                latest_update = await conn.fetchval(
                    "SELECT MAX(updated_at) FROM vector_embeddings"
                )
                stats['latest_update'] = latest_update.isoformat() if latest_update else None
                
                return stats
                
        except Exception as e:
            logger.error(f"âŒ è·å–å‘é‡ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    async def cleanup_old_vectors(self, days: int = 30) -> int:
        """æ¸…ç†æ—§çš„å‘é‡æ•°æ®"""
        try:
            async with self.db_pool.acquire() as conn:
                # åˆ é™¤è¶…è¿‡æŒ‡å®šå¤©æ•°çš„å‘é‡æ•°æ®
                result = await conn.execute("""
                    DELETE FROM vector_embeddings 
                    WHERE updated_at < CURRENT_TIMESTAMP - INTERVAL '$1 days'
                """, days)
                
                deleted_count = int(result.split()[-1])
                logger.info(f"âœ… æ¸…ç†äº† {deleted_count} ä¸ªæ—§å‘é‡")
                return deleted_count
                
        except Exception as e:
            logger.error(f"âŒ æ¸…ç†æ—§å‘é‡å¤±è´¥: {e}")
            return 0
    
    async def rebuild_indexes(self) -> bool:
        """é‡å»ºå‘é‡ç´¢å¼•"""
        try:
            async with self.db_pool.acquire() as conn:
                # åˆ é™¤ç°æœ‰ç´¢å¼•
                await conn.execute("DROP INDEX IF EXISTS idx_resume_vectors_embedding")
                await conn.execute("DROP INDEX IF EXISTS idx_job_vectors_embedding")
                await conn.execute("DROP INDEX IF EXISTS idx_vector_embeddings_embedding")
                
                # é‡æ–°åˆ›å»ºç´¢å¼•
                await self._create_vector_indexes()
                
                logger.info("âœ… å‘é‡ç´¢å¼•é‡å»ºå®Œæˆ")
                return True
                
        except Exception as e:
            logger.error(f"âŒ é‡å»ºå‘é‡ç´¢å¼•å¤±è´¥: {e}")
            return False
